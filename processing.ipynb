{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux/.local/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:397: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "#credentials pour se connecter a ELASTICSEARCH\n",
    "elastic_user = \"elastic\"\n",
    "elastic_password = \"2Tt_dVh8htQwAtlz5-7I\"\n",
    "\n",
    "# Créez une instance d'Elasticsearch avec un pool de connexions\n",
    "es = Elasticsearch(\n",
    "    hosts=[{'host': 'localhost', 'port': 9200, 'scheme': 'https'}],\n",
    "    basic_auth=(elastic_user, elastic_password),\n",
    "    verify_certs=False,\n",
    "    request_timeout=1000000,\n",
    "    connections_per_node=64\n",
    ")\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.SecurityWarning)\n",
    "\n",
    "from elasticsearch.helpers import scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616867\n"
     ]
    }
   ],
   "source": [
    "def get_all_data_from_index():\n",
    "    # recherche de tous nos résultats par 1000 par pagination\n",
    "    query = {\n",
    "        \"query\": {\"match_all\": {}},\n",
    "        \"size\": 1000\n",
    "    }\n",
    "    index_name = 'xml_data' \n",
    "    results = es.search(index=index_name, body=query, scroll='1m')\n",
    "    scroll_id = results['_scroll_id']\n",
    "    scroll_size = results['hits']['total']['value']\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    while scroll_size > 0:\n",
    "        for hit in results['hits']['hits']:\n",
    "            all_data.append(hit[\"_source\"])\n",
    "        \n",
    "        # Requête suivante en utilisant le scroll_id\n",
    "        results = es.scroll(scroll_id=scroll_id, scroll='1m')\n",
    "        \n",
    "        # Mettre à jour le scroll_id et le scroll_size\n",
    "        scroll_id = results['_scroll_id']\n",
    "        scroll_size = len(results['hits']['hits'])\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "data = get_all_data_from_index()\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de convertir les données en dataframe\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permet de définir toutes les colonnes nécéssitant d'être transformer\n",
    "all_categoricals = ['appName', 'direction', 'sourceTCPFlagsDescription', 'destinationTCPFlagsDescription', 'protocolName', 'source', 'destination', 'sourcePort', 'destinationPort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisez le LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "#on trasnforme toutes les données qui peuvent être transformées en binaires (celles qui ne possèdent pas beaucoup de valeur différentes)\n",
    "binarizer = label_binarizer.fit_transform(df['appName'])\n",
    "df_binarizer = pd.DataFrame(data=binarizer,columns=label_binarizer.classes_)\n",
    "\n",
    "#on rajoute nos nouvelles données \n",
    "df = pd.concat([df, df_binarizer], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "# nous faisont ci-dessous une fonction permettant de transformer les addresses IP en entier\n",
    "def ipv4_to_int (ip_str):\n",
    "    try:\n",
    "        # permet de passer notre string d'ip en objet ipaddress\n",
    "        ip_obj = ipaddress.IPv4Address(ip_str)\n",
    "        #par la suite on représente ce nouvel objet en entier\n",
    "        ip_int = int(ip_obj)\n",
    "        return ip_int\n",
    "    except ipaddress.AddressValueError:\n",
    "        print(f\"L'ip n'a pas était passé en entier: {ip_str}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'une nouvelle colonne dans le but de d'associé stopDateTime et startDateTime dans un format plus exploitable\n",
    "df['duration'] = pd.to_datetime(df['stopDateTime']) - pd.to_datetime(df['startDateTime'])\n",
    "if df['duration'].dtypes == 'timedelta64[ns]':\n",
    "    df['duration'] = df['duration'].dt.total_seconds()\n",
    "   \n",
    "#on transforme nos string en valeur numérique \n",
    "if df['totalSourceBytes'].dtypes == 'object':\n",
    "    df['totalSourceBytes'] = pd.to_numeric(df['totalSourceBytes'], errors='coerce')\n",
    "\n",
    "if df['totalDestinationBytes'].dtypes == 'object':\n",
    "    df['totalDestinationBytes'] = pd.to_numeric(df['totalDestinationBytes'], errors='coerce')\n",
    "\n",
    "if df['totalSourcePackets'].dtypes == 'object':\n",
    "    df['totalSourcePackets'] = pd.to_numeric(df['totalSourcePackets'], errors='coerce')\n",
    "\n",
    "if df['totalDestinationPackets'].dtypes == 'object':\n",
    "    df['totalDestinationPackets'] = pd.to_numeric(df['totalDestinationPackets'], errors='coerce')\n",
    "\n",
    "#transformation des ip en int \n",
    "df['destination_int'] = df['destination'].apply(ipv4_to_int)\n",
    "df['source_int'] = df['source'].apply(ipv4_to_int)\n",
    "df['port_source'] = df['sourcePort'].apply(int)\n",
    "df['port_destination'] = df['destinationPort'].apply(int)\n",
    "\n",
    "\n",
    "#par la suite on va séparér les durées par 7 intervalles différents  \n",
    "quantile = pd.cut(df['duration'], bins=7)\n",
    "df['duration_quantile'] = quantile\n",
    "\n",
    "#on sépare ici en intervalles dans le but de prendre moins de colonnes lors du one hot encoding \n",
    "quantile = pd.qcut(df['totalSourceBytes'], q=8)\n",
    "df['totalSourceBytes_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['totalDestinationBytes'], q=9) \n",
    "df['totalDestinationBytes_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['totalSourcePackets'], q=6,duplicates='drop')\n",
    "df['totalSourcePackets_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['totalDestinationPackets'], q=7)\n",
    "df['totalDestinationPackets_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['source_int'], q=6,duplicates='drop')\n",
    "df['source_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['destination_int'], q=6,duplicates='drop')\n",
    "df['destination_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['port_source'], q=50,duplicates='drop')\n",
    "df['portSource_quantile'] = quantile\n",
    "\n",
    "quantile = pd.qcut(df['port_destination'], q=50,duplicates='drop')\n",
    "df['portDestination_quantile'] = quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocolName\n",
      "direction\n",
      "duration_quantile\n",
      "source_quantile\n",
      "totalDestinationPackets_quantile\n",
      "destination_quantile\n",
      "totalSourcePackets_quantile\n",
      "totalSourceBytes_quantile\n",
      "totalDestinationBytes_quantile\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize a dictionary to store OneHotEncoder objects for each categorical column.\n",
    "onehot_encoders = {}\n",
    "\n",
    "# Convert the 'Tag' column to binary: 0 if the value is 'Normal', and 1 otherwise (indicating an anomaly or attack).\n",
    "df['Tag'] = df['Tag'].apply(lambda x: 0 if x == 'Normal' else 1)\n",
    "\n",
    "# On définit les colonnes que l'on va transformer via le one hot encoding\n",
    "categorical_columns = [\n",
    "    'protocolName', 'direction', 'duration_quantile', 'source_quantile',\n",
    "    'totalDestinationPackets_quantile', 'destination_quantile',\n",
    "    'totalSourcePackets_quantile', 'totalSourceBytes_quantile',\n",
    "    'totalDestinationBytes_quantile'\n",
    "]\n",
    "\n",
    "# Initialize the OneHotEncoder.\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# One-hot encode the specified categorical columns.\n",
    "for column in categorical_columns:\n",
    "    print(column)\n",
    "    # Fit and transform the column using the encoder.\n",
    "    encoded_cols = encoder.fit_transform(df[[column]].astype(str))\n",
    "    \n",
    "    # Create new column names for the encoded columns.\n",
    "    col_names = [column + '_' + str(index) for index, _ in enumerate(encoder.categories_[0][1:], start=1)]\n",
    "    \n",
    "    # Convert the encoded columns to a dataframe.\n",
    "    encoded_df = pd.DataFrame(encoded_cols, columns=col_names)\n",
    "    \n",
    "    # Concatenate the encoded dataframe with the original dataframe.\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "    # Drop the original categorical column.\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    # Store the encoder object in the dictionary for possible future use.\n",
    "    onehot_encoders[column] = encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On créer maintenant des colonnes dans le but de réprésenter la colonne destinationTCPFlagsDescription ainsi que source on représentera ces colonne à la manières de one hot encoding\n",
    "\n",
    "#on mets un 1 losque l'on retrouve le tag sinon on met 0 \n",
    "\n",
    "df['destinationTCPFlagsR'] = df['destinationTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'R' in x else 0))\n",
    "df['destinationTCPFlagsS'] = df['destinationTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'S' in x else 0))\n",
    "df['destinationTCPFlagsF'] = df['destinationTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'F' in x else 0))\n",
    "df['destinationTCPFlagsP'] = df['destinationTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'P' in x else 0))\n",
    "df['destinationTCPFlagsA'] = df['destinationTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'A' in x else 0))\n",
    "\n",
    "df['sourceTCPFlagsTagR'] = df['sourceTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'R' in x else 0))\n",
    "df['sourceTCPFlagsTagS'] = df['sourceTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'S' in x else 0))\n",
    "df['sourceTCPFlagsTagF'] = df['sourceTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'F' in x else 0))\n",
    "df['sourceTCPFlagsTagP'] = df['sourceTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'P' in x else 0))\n",
    "df['sourceTCPFlagsTagA'] = df['sourceTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'A' in x else 0))\n",
    "df['sourceTCPFlagsTagU'] = df['sourceTCPFlagsDescription'].apply(lambda x: 0 if x is None or 'N/A' in x else (1 if 'U' in x else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#on retire toutes les colonnes que l'on a plus besoin\n",
    "df.drop('totalDestinationBytes', axis=1, inplace=True)\n",
    "df.drop('totalDestinationPackets', axis=1, inplace=True)\n",
    "df.drop('startDateTime', axis=1, inplace=True)\n",
    "df.drop('stopDateTime', axis=1, inplace=True)\n",
    "df.drop('sourcePayloadAsUTF', axis=1, inplace=True)\n",
    "df.drop('startTime', axis=1, inplace=True)\n",
    "df.drop('duration', axis=1, inplace=True)\n",
    "df.drop('sourceTCPFlagsDescription', axis=1, inplace=True)\n",
    "df.drop('destinationTCPFlagsDescription', axis=1, inplace=True)\n",
    "df.drop('source', axis=1, inplace=True)\n",
    "df.drop('destination', axis=1, inplace=True)\n",
    "df.drop('origin', axis=1, inplace=True)\n",
    "df.drop('appName', axis=1, inplace=True)\n",
    "df.drop('sourcePort', axis=1, inplace=True)\n",
    "df.drop('sensorInterfaceId', axis=1, inplace=True)\n",
    "df.drop('destinationPort', axis=1, inplace=True)\n",
    "df.drop('totalSourceBytes', axis=1, inplace=True)\n",
    "df.drop('destination_int', axis=1, inplace=True)\n",
    "df.drop('source_int', axis=1, inplace=True)\n",
    "df.drop('totalSourcePackets', axis=1, inplace=True)\n",
    "df.drop('sourcePayloadAsBase64', axis=1, inplace=True)\n",
    "df.drop('destinationPayloadAsBase64', axis=1, inplace=True)\n",
    "df.drop('destinationPayloadAsUTF', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tag', 'AOL-ICQ', 'Anet', 'Authentication', 'BGP', 'BitTorrent', 'Blubster', 'Citrix', 'Common-P2P-Port', 'Common-Ports', 'DNS', 'DNS-Port', 'FTP', 'Filenet', 'Flowgen', 'Gnutella', 'Google', 'Groove', 'GuptaSQLBase', 'H.323', 'HTTPImageTransfer', 'HTTPWeb', 'Hosts2-Ns', 'Hotline', 'ICMP', 'IGMP', 'IMAP', 'IPSec', 'IPX', 'IRC', 'Ingres', 'Intellex', 'Kazaa', 'LDAP', 'MDQS', 'MGCP', 'MS-SQL', 'MSMQ', 'MSN', 'MSN-Zone', 'MSTerminalServices', 'ManagementServices', 'MicrosoftMediaServer', 'Misc-DB', 'Misc-Mail-Port', 'Misc-Ports', 'MiscApp', 'MiscApplication', 'NETBEUI', 'NFS', 'NNTPNews', 'NTP', 'Nessus', 'NetBIOS-IP', 'Network-Config-Ports', 'NortonAntiVirus', 'NortonGhost', 'OpenNap', 'OpenWindows', 'Oracle', 'PCAnywhere', 'POP', 'POP-port', 'PPTP', 'PeerEnabler', 'PostgreSQL', 'Printer', 'RPC', 'RTSP', 'Real', 'SAP', 'SIP', 'SMS', 'SMTP', 'SNA', 'SNMP-Ports', 'SSDP', 'SSH', 'SSL-Shell', 'SecureWeb', 'Squid', 'StreamingAudio', 'SunRPC', 'Tacacs', 'Telnet', 'Timbuktu', 'TimeServer', 'Unknown_TCP', 'Unknown_UDP', 'UpdateDaemon', 'VNC', 'Web-Port', 'WebFileTransfer', 'WebMediaAudio', 'WebMediaDocuments', 'WebMediaVideo', 'Webmin', 'WindowsFileSharing', 'XFER', 'XWindows', 'Yahoo', 'dsp3270', 'giop-ssl', 'iChat', 'rexec', 'rlogin', 'rsh', 'port_source', 'port_destination', 'portSource_quantile', 'portDestination_quantile', 'protocolName_1', 'protocolName_2', 'protocolName_3', 'protocolName_4', 'protocolName_5', 'direction_1', 'direction_2', 'direction_3', 'duration_quantile_1', 'duration_quantile_2', 'duration_quantile_3', 'duration_quantile_4', 'source_quantile_1', 'source_quantile_2', 'source_quantile_3', 'source_quantile_4', 'source_quantile_5', 'totalDestinationPackets_quantile_1', 'totalDestinationPackets_quantile_2', 'totalDestinationPackets_quantile_3', 'totalDestinationPackets_quantile_4', 'totalDestinationPackets_quantile_5', 'totalDestinationPackets_quantile_6', 'destination_quantile_1', 'destination_quantile_2', 'destination_quantile_3', 'destination_quantile_4', 'destination_quantile_5', 'totalSourcePackets_quantile_1', 'totalSourcePackets_quantile_2', 'totalSourcePackets_quantile_3', 'totalSourcePackets_quantile_4', 'totalSourceBytes_quantile_1', 'totalSourceBytes_quantile_2', 'totalSourceBytes_quantile_3', 'totalSourceBytes_quantile_4', 'totalSourceBytes_quantile_5', 'totalSourceBytes_quantile_6', 'totalSourceBytes_quantile_7', 'totalDestinationBytes_quantile_1', 'totalDestinationBytes_quantile_2', 'totalDestinationBytes_quantile_3', 'totalDestinationBytes_quantile_4', 'totalDestinationBytes_quantile_5', 'totalDestinationBytes_quantile_6', 'totalDestinationBytes_quantile_7', 'totalDestinationBytes_quantile_8', 'destinationTCPFlagsR', 'destinationTCPFlagsS', 'destinationTCPFlagsF', 'destinationTCPFlagsP', 'destinationTCPFlagsA', 'sourceTCPFlagsTagR', 'sourceTCPFlagsTagS', 'sourceTCPFlagsTagF', 'sourceTCPFlagsTagP', 'sourceTCPFlagsTagA', 'sourceTCPFlagsTagU']\n"
     ]
    }
   ],
   "source": [
    "#permet de stocker les élements transformées dans un fichier pickle\n",
    "print(df.columns.to_list())\n",
    "df.to_pickle('categorical_transform.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
